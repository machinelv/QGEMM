{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3d1f4e",
   "metadata": {},
   "source": [
    "# QGEMM Benchmark Notebook\n",
    "\n",
    "This notebook provides comprehensive benchmarking for QGEMM kernels including:\n",
    "- Performance comparison with PyTorch and vendor libraries (cuBLAS/rocBLAS)\n",
    "- Testing custom GEMM kernels (including CUTLASS integration)\n",
    "- Visualization of results and performance analysis\n",
    "- Correctness verification of custom kernels\n",
    "\n",
    "## Requirements\n",
    "- PyTorch with CUDA/ROCm support\n",
    "- Build QGEMM with `-DBUILD_TESTS=ON`\n",
    "- For test kernels: `-DBUILD_TEST_KERNELS=ON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456eb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Add build directory to path for loading compiled modules\n",
    "sys.path.append('../build/lib')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Memory info\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {memory_allocated:.2f} GB, Reserved: {memory_reserved:.2f} GB, Total: {memory_total:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import test kernels if available\n",
    "test_kernels_available = False\n",
    "try:\n",
    "    import qgemm_test_kernels_python\n",
    "    test_kernels_available = True\n",
    "    print(\"‚úÖ Test kernels loaded successfully\")\n",
    "    print(\"   Available functions:\", dir(qgemm_test_kernels_python))\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Test kernels not available: {e}\")\n",
    "    print(\"   Build with -DBUILD_TEST_KERNELS=ON to enable test kernels\")\n",
    "\n",
    "# Try to import main QGEMM module\n",
    "qgemm_available = False\n",
    "try:\n",
    "    import qgemm_python\n",
    "    qgemm_available = True\n",
    "    print(\"‚úÖ QGEMM Python module loaded successfully\")\n",
    "    print(\"   Available functions:\", dir(qgemm_python))\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  QGEMM Python module not available: {e}\")\n",
    "    print(\"   Make sure the project is built and the build/lib directory is in your path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f781e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmBenchmark:\n",
    "    \"\"\"GEMM Benchmark utility class\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cuda', warmup_runs=5, benchmark_runs=10):\n",
    "        self.device = device\n",
    "        self.warmup_runs = warmup_runs\n",
    "        self.benchmark_runs = benchmark_runs\n",
    "        self.results = []\n",
    "    \n",
    "    def benchmark_function(self, func, *args, **kwargs):\n",
    "        \"\"\"Benchmark a function with warmup and multiple runs\"\"\"\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(self.warmup_runs):\n",
    "            result = func(*args, **kwargs)\n",
    "            if self.device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(self.benchmark_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            result = func(*args, **kwargs)\n",
    "            if self.device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            times.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "        \n",
    "        return np.mean(times), np.std(times), result\n",
    "    \n",
    "    def calculate_tflops(self, M, N, K, time_ms):\n",
    "        \"\"\"Calculate TFLOPS from matrix dimensions and time\"\"\"\n",
    "        flops = 2 * M * N * K  # 2 operations per multiply-add\n",
    "        time_s = time_ms / 1000.0\n",
    "        return (flops / time_s) / 1e12\n",
    "    \n",
    "    def test_pytorch_gemm(self, M, N, K, dtype=torch.float16):\n",
    "        \"\"\"Test PyTorch GEMM\"\"\"\n",
    "        A = torch.randn(M, K, dtype=dtype, device=self.device)\n",
    "        B = torch.randn(K, N, dtype=dtype, device=self.device)\n",
    "        \n",
    "        avg_time, std_time, result = self.benchmark_function(\n",
    "            torch.mm, A, B\n",
    "        )\n",
    "        \n",
    "        tflops = self.calculate_tflops(M, N, K, avg_time)\n",
    "        \n",
    "        return {\n",
    "            'kernel': 'PyTorch',\n",
    "            'M': M, 'N': N, 'K': K,\n",
    "            'dtype': str(dtype).split('.')[-1],\n",
    "            'time_ms': avg_time,\n",
    "            'std_ms': std_time,\n",
    "            'tflops': tflops\n",
    "        }\n",
    "    \n",
    "    def test_cutlass_gemm(self, M, N, K, dtype=torch.float16):\n",
    "        \"\"\"Test CUTLASS GEMM if available\"\"\"\n",
    "        if not test_kernels_available:\n",
    "            return None\n",
    "            \n",
    "        A = torch.randn(M, K, dtype=dtype, device=self.device)\n",
    "        B = torch.randn(K, N, dtype=dtype, device=self.device)\n",
    "        C = torch.zeros(M, N, dtype=dtype, device=self.device)\n",
    "        \n",
    "        try:\n",
    "            avg_time, std_time, result = self.benchmark_function(\n",
    "                qgemm_test_kernels_python.cutlass_gemm, A, B, C\n",
    "            )\n",
    "            \n",
    "            tflops = self.calculate_tflops(M, N, K, avg_time)\n",
    "            \n",
    "            return {\n",
    "                'kernel': 'CUTLASS',\n",
    "                'M': M, 'N': N, 'K': K,\n",
    "                'dtype': str(dtype).split('.')[-1],\n",
    "                'time_ms': avg_time,\n",
    "                'std_ms': std_time,\n",
    "                'tflops': tflops\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"CUTLASS test failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_comprehensive_benchmark(self):\n",
    "        \"\"\"Run comprehensive benchmark across different sizes and precisions\"\"\"\n",
    "        \n",
    "        # Test configurations\n",
    "        sizes = [(512, 512, 512), (1024, 1024, 1024), (2048, 2048, 2048), (4096, 4096, 4096)]\n",
    "        dtypes = [torch.float16, torch.float32]\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "        for M, N, K in sizes:\n",
    "            for dtype in dtypes:\n",
    "                print(f\"Testing M={M}, N={N}, K={K}, dtype={dtype}...\")\n",
    "                \n",
    "                # Test PyTorch\n",
    "                pytorch_result = self.test_pytorch_gemm(M, N, K, dtype)\n",
    "                self.results.append(pytorch_result)\n",
    "                \n",
    "                # Test CUTLASS if available\n",
    "                if test_kernels_available and dtype == torch.float16:\n",
    "                    cutlass_result = self.test_cutlass_gemm(M, N, K, dtype)\n",
    "                    if cutlass_result:\n",
    "                        self.results.append(cutlass_result)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def get_results_dataframe(self):\n",
    "        \"\"\"Convert results to pandas DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = GemmBenchmark()\n",
    "print(\"‚úÖ Benchmark framework initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive benchmark\n",
    "print(\"üöÄ Starting comprehensive benchmark...\")\n",
    "print(\"This may take several minutes depending on your GPU...\")\n",
    "\n",
    "try:\n",
    "    results = benchmark.run_comprehensive_benchmark()\n",
    "    print(f\"\\n‚úÖ Completed {len(results)} benchmark tests\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Benchmark failed: {e}\")\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "df = benchmark.get_results_dataframe()\n",
    "if not df.empty:\n",
    "    print(\"\\nüìä === Benchmark Results ===\")\n",
    "    pd.set_option('display.precision', 3)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìà === Performance Summary ===\")\n",
    "    for kernel in df['kernel'].unique():\n",
    "        kernel_data = df[df['kernel'] == kernel]\n",
    "        avg_tflops = kernel_data['tflops'].mean()\n",
    "        max_tflops = kernel_data['tflops'].max()\n",
    "        print(f\"{kernel}: Avg {avg_tflops:.2f} TFLOPS, Peak {max_tflops:.2f} TFLOPS\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No benchmark results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if not df.empty:\n",
    "    # Performance comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # TFLOPS comparison\n",
    "    for kernel in df['kernel'].unique():\n",
    "        kernel_data = df[df['kernel'] == kernel]\n",
    "        sizes = [f\"{row['M']}\" for _, row in kernel_data.iterrows()]\n",
    "        ax1.plot(range(len(kernel_data)), kernel_data['tflops'], 'o-', label=kernel, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax1.set_xlabel('Test Configuration Index')\n",
    "    ax1.set_ylabel('TFLOPS')\n",
    "    ax1.set_title('Performance Comparison (TFLOPS)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Execution time comparison\n",
    "    for kernel in df['kernel'].unique():\n",
    "        kernel_data = df[df['kernel'] == kernel]\n",
    "        ax2.plot(range(len(kernel_data)), kernel_data['time_ms'], 'o-', label=kernel, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax2.set_xlabel('Test Configuration Index')\n",
    "    ax2.set_ylabel('Execution Time (ms)')\n",
    "    ax2.set_title('Execution Time Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance by matrix size\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    fp16_data = df[df['dtype'] == 'float16']\n",
    "    if not fp16_data.empty:\n",
    "        for kernel in fp16_data['kernel'].unique():\n",
    "            kernel_data = fp16_data[fp16_data['kernel'] == kernel]\n",
    "            plt.plot(kernel_data['M'], kernel_data['tflops'], 'o-', label=f'{kernel} (FP16)', linewidth=2, markersize=8)\n",
    "    \n",
    "    fp32_data = df[df['dtype'] == 'float32']\n",
    "    if not fp32_data.empty:\n",
    "        for kernel in fp32_data['kernel'].unique():\n",
    "            kernel_data = fp32_data[fp32_data['kernel'] == kernel]\n",
    "            plt.plot(kernel_data['M'], kernel_data['tflops'], 's--', label=f'{kernel} (FP32)', linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.xlabel('Matrix Size (M=N=K)')\n",
    "    plt.ylabel('TFLOPS')\n",
    "    plt.title('Performance vs Matrix Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale('log', base=2)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"üìä No data to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2dfa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom kernel correctness\n",
    "def test_kernel_correctness(M=128, N=128, K=128, tolerance=1e-3):\n",
    "    \"\"\"Test kernel correctness against PyTorch reference\"\"\"\n",
    "    \n",
    "    if not test_kernels_available:\n",
    "        print(\"‚ö†Ô∏è  Test kernels not available for correctness testing\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nüîç Testing kernel correctness (M={M}, N={N}, K={K})...\")\n",
    "    \n",
    "    # Create test matrices\n",
    "    A = torch.randn(M, K, dtype=torch.float16, device='cuda')\n",
    "    B = torch.randn(K, N, dtype=torch.float16, device='cuda')\n",
    "    C = torch.zeros(M, N, dtype=torch.float16, device='cuda')\n",
    "    \n",
    "    # PyTorch reference\n",
    "    C_ref = torch.mm(A, B)\n",
    "    \n",
    "    try:\n",
    "        # Custom kernel\n",
    "        qgemm_test_kernels_python.cutlass_gemm(A, B, C)\n",
    "        \n",
    "        # Compare results\n",
    "        diff = torch.abs(C - C_ref)\n",
    "        max_diff = torch.max(diff)\n",
    "        mean_diff = torch.mean(diff)\n",
    "        \n",
    "        print(f\"   Max difference: {max_diff:.6f}\")\n",
    "        print(f\"   Mean difference: {mean_diff:.6f}\")\n",
    "        print(f\"   Tolerance: {tolerance}\")\n",
    "        \n",
    "        if max_diff < tolerance:\n",
    "            print(\"   ‚úÖ Correctness test PASSED\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"   ‚ùå Correctness test FAILED\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Kernel test failed with error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run correctness tests\n",
    "print(\"üß™ === Correctness Testing ===\")\n",
    "test_sizes = [(128, 128, 128), (256, 256, 256), (512, 512, 512)]\n",
    "for M, N, K in test_sizes:\n",
    "    test_kernel_correctness(M, N, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781226a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage analysis\n",
    "def analyze_memory_usage(M, N, K, dtype=torch.float16):\n",
    "    \"\"\"Analyze memory usage for GEMM operation\"\"\"\n",
    "    \n",
    "    dtype_size = 2 if dtype == torch.float16 else 4  # bytes\n",
    "    \n",
    "    memory_A = M * K * dtype_size\n",
    "    memory_B = K * N * dtype_size\n",
    "    memory_C = M * N * dtype_size\n",
    "    \n",
    "    total_memory = memory_A + memory_B + memory_C\n",
    "    \n",
    "    return {\n",
    "        'Matrix A (MB)': memory_A / (1024**2),\n",
    "        'Matrix B (MB)': memory_B / (1024**2),\n",
    "        'Matrix C (MB)': memory_C / (1024**2),\n",
    "        'Total (MB)': total_memory / (1024**2),\n",
    "        'Total (GB)': total_memory / (1024**3)\n",
    "    }\n",
    "\n",
    "# Analyze memory for different sizes\n",
    "print(\"\\nüíæ === Memory Usage Analysis ===\")\n",
    "sizes = [512, 1024, 2048, 4096, 8192]\n",
    "memory_analysis = []\n",
    "\n",
    "for size in sizes:\n",
    "    mem_fp16 = analyze_memory_usage(size, size, size, torch.float16)\n",
    "    mem_fp32 = analyze_memory_usage(size, size, size, torch.float32)\n",
    "    \n",
    "    memory_analysis.append({\n",
    "        'Size': f'{size}x{size}x{size}',\n",
    "        'FP16 (MB)': mem_fp16['Total (MB)'],\n",
    "        'FP32 (MB)': mem_fp32['Total (MB)'],\n",
    "        'FP16 (GB)': mem_fp16['Total (GB)'],\n",
    "        'FP32 (GB)': mem_fp32['Total (GB)']\n",
    "    })\n",
    "\n",
    "memory_df = pd.DataFrame(memory_analysis)\n",
    "print(memory_df.to_string(index=False))\n",
    "\n",
    "# Export results to CSV\n",
    "if not df.empty:\n",
    "    output_file = 'qgemm_benchmark_results.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nüíæ Results exported to {output_file}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nüéØ === Final Summary ===\")\n",
    "    summary = df.groupby(['kernel', 'dtype']).agg({\n",
    "        'tflops': ['mean', 'max', 'std'],\n",
    "        'time_ms': ['mean', 'min', 'std']\n",
    "    }).round(3)\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No results to export\")\n",
    "\n",
    "print(\"\\nüéâ Benchmark notebook completed!\")\n",
    "print(\"\\nüìã Build Instructions:\")\n",
    "print(\"1. cd .. && ./build.sh --tests --vendor=NVIDIA --arch=80,89\")\n",
    "print(\"2. For test kernels: ./build.sh --tests --test-kernels\")\n",
    "print(\"3. Run C++ tests: ./build/bin/qgemm_test\")\n",
    "print(\"4. Run this notebook for Python interface testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c56ee8",
   "metadata": {},
   "source": [
    "# GEMM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict, Optional, Callable\n",
    "import triton\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import triton.language as tl\n",
    "\n",
    "# Define precision types\n",
    "class PrecisionType:\n",
    "    FP32 = \"fp32\"\n",
    "    FP16 = \"fp16\"\n",
    "    BF16 = \"bf16\"\n",
    "    INT8 = \"int8\"\n",
    "    FP8 = \"fp8\"  # Note: FP8 support in PyTorch is experimental\n",
    "\n",
    "class GEMMTester:\n",
    "    def __init__(self):\n",
    "        self.kernels = {}\n",
    "        self.register_default_kernels()\n",
    "        \n",
    "    def register_kernel(self, name: str, kernel_fn: Callable):\n",
    "        \"\"\"Register a GEMM kernel with a name\"\"\"\n",
    "        self.kernels[name] = kernel_fn\n",
    "        \n",
    "    def register_default_kernels(self):\n",
    "        \"\"\"Register default PyTorch kernel\"\"\"\n",
    "        self.register_kernel(\"pytorch\", self._pytorch_gemm)\n",
    "        # Register Triton kernel\n",
    "        self.register_kernel(\"triton\", self._triton_gemm)\n",
    "        # We'll assume C++ kernels will be registered separately via pybind\n",
    "        \n",
    "    def _pytorch_gemm(self, A, B, C, precision: str):\n",
    "        \"\"\"PyTorch native GEMM implementation: D = AB + C\"\"\"\n",
    "        if precision == PrecisionType.FP32:\n",
    "            A = A.to(torch.float32)\n",
    "            B = B.to(torch.float32)\n",
    "            C = C.to(torch.float32)\n",
    "        elif precision == PrecisionType.FP16:\n",
    "            A = A.to(torch.float16)\n",
    "            B = B.to(torch.float16)\n",
    "            C = C.to(torch.float16)\n",
    "        elif precision == PrecisionType.BF16:\n",
    "            A = A.to(torch.bfloat16)\n",
    "            B = B.to(torch.bfloat16)\n",
    "            C = C.to(torch.bfloat16)\n",
    "        elif precision == PrecisionType.INT8:\n",
    "            A = A.to(torch.int8)\n",
    "            B = B.to(torch.int8)\n",
    "            C = C.to(torch.int8)\n",
    "        # FP8 support is experimental and may require special handling\n",
    "        \n",
    "        return torch.matmul(A, B) + C\n",
    "    \n",
    "    @staticmethod\n",
    "    @triton.jit\n",
    "    def _triton_matmul_kernel(\n",
    "        a_ptr, b_ptr, c_ptr, d_ptr,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        stride_dm, stride_dn,\n",
    "        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n",
    "    ):\n",
    "        \"\"\"Triton kernel for GEMM operation\"\"\"\n",
    "        pid_m = tl.program_id(0)\n",
    "        pid_n = tl.program_id(1)\n",
    "        \n",
    "        # Compute block offsets\n",
    "        m_start = pid_m * BLOCK_M\n",
    "        n_start = pid_n * BLOCK_N\n",
    "        \n",
    "        # Initialize accumulator\n",
    "        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "        \n",
    "        # Iterate over blocks of K\n",
    "        for k in range(0, K, BLOCK_K):\n",
    "            # Load A block\n",
    "            a_block_ptr = a_ptr + m_start * stride_am + k * stride_ak\n",
    "            a_block = tl.load(a_block_ptr, mask=tl.arange(0, BLOCK_M)[:, None] < M - m_start, other=0.0)\n",
    "            \n",
    "            # Load B block\n",
    "            b_block_ptr = b_ptr + k * stride_bk + n_start * stride_bn\n",
    "            b_block = tl.load(b_block_ptr, mask=tl.arange(0, BLOCK_N)[None, :] < N - n_start, other=0.0)\n",
    "            \n",
    "            # Compute matmul for this block\n",
    "            acc += tl.dot(a_block, b_block)\n",
    "        \n",
    "        # Load C block\n",
    "        c_block_ptr = c_ptr + m_start * stride_cm + n_start * stride_cn\n",
    "        c_block = tl.load(c_block_ptr, mask=(tl.arange(0, BLOCK_M)[:, None] < M - m_start) & \n",
    "                                          (tl.arange(0, BLOCK_N)[None, :] < N - n_start), other=0.0)\n",
    "        \n",
    "        # Add C to the result\n",
    "        result = acc + c_block\n",
    "        \n",
    "        # Store the result\n",
    "        d_block_ptr = d_ptr + m_start * stride_dm + n_start * stride_dn\n",
    "        tl.store(d_block_ptr, result, mask=(tl.arange(0, BLOCK_M)[:, None] < M - m_start) & \n",
    "                                       (tl.arange(0, BLOCK_N)[None, :] < N - n_start))\n",
    "    \n",
    "    def _triton_gemm(self, A, B, C, precision: str):\n",
    "        \"\"\"Use Triton kernel for GEMM: D = AB + C\"\"\"\n",
    "        # Convert to desired precision\n",
    "        if precision == PrecisionType.FP32:\n",
    "            dtype = torch.float32\n",
    "        elif precision == PrecisionType.FP16:\n",
    "            dtype = torch.float16\n",
    "        elif precision == PrecisionType.BF16:\n",
    "            dtype = torch.bfloat16\n",
    "        elif precision == PrecisionType.INT8:\n",
    "            dtype = torch.int8\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported precision: {precision}\")\n",
    "        \n",
    "        A = A.to(dtype)\n",
    "        B = B.to(dtype)\n",
    "        C = C.to(dtype)\n",
    "        \n",
    "        M, K = A.shape\n",
    "        K, N = B.shape\n",
    "        \n",
    "        # Create output tensor\n",
    "        D = torch.empty((M, N), device=A.device, dtype=dtype)\n",
    "        \n",
    "        # Define the grid\n",
    "        grid = (triton.cdiv(M, 32), triton.cdiv(N, 32))\n",
    "        \n",
    "        # Launch the Triton kernel\n",
    "        self._triton_matmul_kernel[grid](\n",
    "            A, B, C, D,\n",
    "            M, N, K,\n",
    "            A.stride(0), A.stride(1),\n",
    "            B.stride(0), B.stride(1),\n",
    "            C.stride(0), C.stride(1),\n",
    "            D.stride(0), D.stride(1),\n",
    "            BLOCK_M=32, BLOCK_N=32, BLOCK_K=32\n",
    "        )\n",
    "        \n",
    "        return D\n",
    "    \n",
    "    def benchmark(self, \n",
    "                 kernel_name: str, \n",
    "                 M: int, N: int, K: int, \n",
    "                 precision: str, \n",
    "                 device: str = \"cuda\",\n",
    "                 num_runs: int = 10):\n",
    "        \"\"\"Benchmark a specific GEMM kernel\"\"\"\n",
    "        if kernel_name not in self.kernels:\n",
    "            raise ValueError(f\"Unknown kernel: {kernel_name}\")\n",
    "        \n",
    "        # Create matrices\n",
    "        A = torch.randn((M, K), device=device)\n",
    "        B = torch.randn((K, N), device=device)\n",
    "        C = torch.randn((M, N), device=device)\n",
    "        \n",
    "        # Warmup\n",
    "        kernel_fn = self.kernels[kernel_name]\n",
    "        for _ in range(5):\n",
    "            D = kernel_fn(A, B, C, precision)\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            D = kernel_fn(A, B, C, precision)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            times.append((end - start) * 1000)  # Convert to ms\n",
    "            \n",
    "        return {\n",
    "            \"kernel\": kernel_name,\n",
    "            \"precision\": precision,\n",
    "            \"M\": M, \"N\": N, \"K\": K,\n",
    "            \"mean_time_ms\": np.mean(times),\n",
    "            \"std_time_ms\": np.std(times),\n",
    "            \"min_time_ms\": np.min(times),\n",
    "            \"max_time_ms\": np.max(times),\n",
    "        }\n",
    "    \n",
    "    def compare_kernels(self, \n",
    "                       kernels: List[str], \n",
    "                       shapes: List[Tuple[int, int, int]], \n",
    "                       precisions: List[str],\n",
    "                       device: str = \"cuda\",\n",
    "                       num_runs: int = 10):\n",
    "        \"\"\"Compare multiple kernels across different shapes and precisions\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for kernel in kernels:\n",
    "            for precision in precisions:\n",
    "                for M, N, K in shapes:\n",
    "                    try:\n",
    "                        result = self.benchmark(kernel, M, N, K, precision, device, num_runs)\n",
    "                        results.append(result)\n",
    "                        print(f\"Kernel: {kernel}, Precision: {precision}, Shape: ({M}, {N}, {K}), \"\n",
    "                              f\"Time: {result['mean_time_ms']:.2f} ¬± {result['std_time_ms']:.2f} ms\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error benchmarking {kernel} with {precision} for shape ({M}, {N}, {K}): {e}\")\n",
    "                        \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, results, plot_type=\"bar\"):\n",
    "        \"\"\"Plot benchmark results\"\"\"\n",
    "        if plot_type == \"bar\":\n",
    "            # Group by shape and precision\n",
    "            data = {}\n",
    "            for r in results:\n",
    "                shape_key = f\"({r['M']}, {r['N']}, {r['K']})\"\n",
    "                prec_key = r['precision']\n",
    "                if shape_key not in data:\n",
    "                    data[shape_key] = {}\n",
    "                if prec_key not in data[shape_key]:\n",
    "                    data[shape_key][prec_key] = {}\n",
    "                data[shape_key][prec_key][r['kernel']] = r['mean_time_ms']\n",
    "            \n",
    "            # Plot\n",
    "            fig, axes = plt.subplots(len(data), figsize=(12, 4 * len(data)))\n",
    "            if len(data) == 1:\n",
    "                axes = [axes]\n",
    "                \n",
    "            for i, (shape, precisions) in enumerate(data.items()):\n",
    "                ax = axes[i]\n",
    "                ax.set_title(f\"Matrix shape: {shape}\")\n",
    "                \n",
    "                x = np.arange(len(precisions))\n",
    "                width = 0.8 / len(next(iter(precisions.values())))\n",
    "                \n",
    "                for j, (prec, kernels) in enumerate(precisions.items()):\n",
    "                    for k, (kernel, time) in enumerate(kernels.items()):\n",
    "                        ax.bar(x[j] + k * width, time, width, label=f\"{kernel} ({prec})\")\n",
    "                \n",
    "                ax.set_ylabel(\"Time (ms)\")\n",
    "                ax.set_xticks(x + width / 2)\n",
    "                ax.set_xticklabels(precisions.keys())\n",
    "                ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Other plot types can be implemented as needed\n",
    "            pass\n",
    "\n",
    "# Example on how to register a C++ kernel with pybind11\n",
    "# This is a placeholder - you'd need to actually implement the C++ module and binding\n",
    "\"\"\"\n",
    "import gemm_cpp  # This would be your pybind11 module\n",
    "\n",
    "def cpp_gemm(A, B, C, precision):\n",
    "    # Convert PyTorch tensors to the format your C++ code expects\n",
    "    # Call your C++ implementation\n",
    "    # Convert back to PyTorch tensor\n",
    "    if precision == PrecisionType.FP32:\n",
    "        return gemm_cpp.gemm_fp32(A, B, C)\n",
    "    elif precision == PrecisionType.BF16:\n",
    "        return gemm_cpp.gemm_bf16(A, B, C)\n",
    "    # ... other precision types\n",
    "\n",
    "# Register the C++ kernel\n",
    "tester = GEMMTester()\n",
    "tester.register_kernel(\"cpp\", cpp_gemm)\n",
    "\"\"\"\n",
    "\n",
    "# Example usage:\n",
    "def run_gemm_tests():\n",
    "    tester = GEMMTester()\n",
    "    \n",
    "    # Define test configurations\n",
    "    kernels = [\"pytorch\", \"triton\"]  # Add \"cpp\" when you have the C++ implementation\n",
    "    shapes = [(128, 128, 128), (512, 512, 512), (1024, 1024, 1024)]\n",
    "    precisions = [PrecisionType.FP32, PrecisionType.FP16, PrecisionType.BF16]\n",
    "    \n",
    "    # For smaller tests, you can use CPU, but for real benchmarks, use CUDA\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Run benchmarks\n",
    "    print(f\"Running GEMM benchmarks on {device}...\")\n",
    "    results = tester.compare_kernels(kernels, shapes, precisions, device)\n",
    "    \n",
    "    # Plot results\n",
    "    tester.plot_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# When running the notebook, call this function to execute the tests\n",
    "# results = run_gemm_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
