# name: fp8-matmul

files:
  - {"name": "submission.py", "source": "@SUBMISSION@"}
  - {"name": "task.py", "source": "task.py"}
  - {"name": "utils.py", "source": "utils.py"}
  - {"name": "reference.py", "source": "reference.py"}
  - {"name": "eval.py", "source": "eval.py"}

lang: "py"

description: |
  
  You will implement a custom fp8-blockwise matmul kernel optimized for MI300.
  You will be given single-precision scaling factors for your matrices.
  The shapes of all outer and inner dimensions of tensors are from DeepSeek-R1.
  To be explicit, you will be given a tuple of tensors:
  ```
  (a, b, a_scale, b_scale, c)
  ```
  where `a` and `b` are the input matrices, `a_scale` and `b_scale` are the scaling factors for `a` and `b` respectively,
  and `c` is the output matrix:
  * `a` is M x K in column-major order in e4m3fnuz
  * `b` is N x K in column-major order in e4m3fnuz
  * `a_scale` is M x K in column-major order in fp32
  * `b_scale` is N x K in column-major order in fp32
  * `c` is M x N in ROW-major order in bf16
  
  Matrix sizes `m` and `n` are divisible by 64, `k` is divisible by 128.

  The ranking criteria is the geometric mean of the benchmark results.

  For the grand price, your kernel will be evaluated against the speed of light analysis
  and the solution closest to the speed of light will be awarded the grand price.
  ```
  The speed of light analysis is:
   M       N       K     time[us]
  1024    1536    7168      8.63
  1024    4608    7168     25.89
  6144    1536    7168     51.78
  6144    4608    7168    155.30
  1024    7168     256      3.17
  6144    7168     256     17.27
  ```

config:
  main: "eval.py"

templates:
  Python: "template.py"
  HIP: "template-hip.py"

tests:
  - {"m": 64, "n": 64, "k": 128, "seed": 6635}
  - {"m": 64, "n": 1536, "k": 7168, "seed": 6635}
  - {"m": 64, "n": 3072, "k": 1536, "seed": 1236}
  - {"m": 64, "n": 576, "k": 7168, "seed": 542}
  - {"m": 96, "n": 7168, "k": 256, "seed": 1234}
  - {"m": 96, "n": 7168, "k": 2048, "seed": 4153}
  - {"m": 96, "n": 4608, "k": 7168, "seed": 412}
  - {"m": 128, "n": 7168, "k": 2304, "seed": 624}
  - {"m": 128, "n": 512, "k": 7168, "seed": 2514}
  - {"m": 512, "n": 4096, "k": 512, "seed": 543}
  - {"m": 512, "n": 1536, "k": 7168, "seed": 12341}

benchmarks:
  - {"m": 1024, "n": 1536, "k": 7168, "seed": 8135}
  - {"m": 1024, "n": 3072, "k": 1536, "seed": 6251}
  - {"m": 1024, "n": 576, "k": 7168, "seed": 12346}
  - {"m": 1024, "n": 7168, "k": 256, "seed": 5364}
  - {"m": 1024, "n": 7168, "k": 2048, "seed": 6132}
  - {"m": 1024, "n": 4608, "k": 7168, "seed": 7531}
  - {"m": 1024, "n": 7168, "k": 2304, "seed": 12345}
  - {"m": 1024, "n": 512, "k": 7168, "seed": 6563}
  - {"m": 1024, "n": 4096, "k": 512, "seed": 17512}
  - {"m": 6144, "n": 1536, "k": 7168, "seed": 6543}
  - {"m": 6144, "n": 3072, "k": 1536, "seed": 234}
  - {"m": 6144, "n": 576, "k": 7168, "seed": 9863}
  - {"m": 6144, "n": 7168, "k": 256, "seed": 764243}
  - {"m": 6144, "n": 7168, "k": 2048, "seed": 76547}
  - {"m": 6144, "n": 4608, "k": 7168, "seed": 65436}
  - {"m": 6144, "n": 7168, "k": 2304, "seed": 452345}
  - {"m": 6144, "n": 512, "k": 7168, "seed": 12341}
  - {"m": 6144, "n": 4096, "k": 512, "seed": 45245}

ranking_by: "geom"
